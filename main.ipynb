{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Handwritten digits recognition using fully connected neural network (Multi layer perception):\n",
        "\n",
        "> This project aims to develop a handwritten digit recognition model using a Multilayer Perceptron (MLP) on the renowned MNIST dataset. MNIST consists of 70,000 grayscale images of digits from 0 to 9, each with a size of 28x28 pixels.\n",
        "\n",
        "\n",
        "In this experiment we will build a [Multilayer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) model using [Tensorflow](https://www.tensorflow.org/) to recognize handwritten digits.\n",
        "\n",
        "A **multilayer perceptron** (MLP) is a class of feedforward artificial neural network. An MLP consists of, at least, three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.\n",
        "\n",
        "![digits_recognition_mlp.png](https://miro.medium.com/v2/resize:fit:720/format:webp/1*hSbHWUSQreLeyA6Nr2KIqg.png)"
      ],
      "metadata": {
        "id": "T0b2-xsgxup0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kaGcgkgYxh7a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_mnist():\n",
        "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Reshape images to include channel dimension (grayscale)\n",
        "    train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
        "    test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
        "\n",
        "    # Convert labels to one-hot encoding\n",
        "    train_labels = tf.keras.utils.to_categorical(train_labels)\n",
        "    test_labels = tf.keras.utils.to_categorical(test_labels)\n",
        "\n",
        "    return (train_images, train_labels), (test_images, test_labels)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    (train_images, train_labels), (test_images, test_labels) = load_and_preprocess_mnist()\n",
        "    input_shape = train_images.shape[1:]\n",
        "    num_classes = train_labels.shape[1]\n",
        "\n",
        "    model = create_cnn_model(input_shape, num_classes)\n",
        "    model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "BDpoiLOo-bOP",
        "outputId": "af443489-0f8a-4960-a100-cb1f4311784d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93,322\u001b[0m (364.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,322</span> (364.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m93,322\u001b[0m (364.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,322</span> (364.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = load_and_preprocess_mnist()\n",
        "input_shape = train_images.shape[1:]\n",
        "num_classes = train_labels.shape[1]\n",
        "\n",
        "model = create_cnn_model(input_shape, num_classes)\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64, validation_data=(test_images, test_labels))\n",
        "\n",
        "model.save('mnist_cnn_model.h5')\n",
        "\n",
        "print('Modèle entraîné et sauvegardé avec succès !')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qyv0pqhNyLVm",
        "outputId": "e8364ea1-b66d-4993-e6ab-7723c1c6f691"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 53ms/step - accuracy: 0.8667 - loss: 0.4344 - val_accuracy: 0.9825 - val_loss: 0.0525\n",
            "Epoch 2/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 51ms/step - accuracy: 0.9840 - loss: 0.0496 - val_accuracy: 0.9888 - val_loss: 0.0353\n",
            "Epoch 3/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 53ms/step - accuracy: 0.9896 - loss: 0.0347 - val_accuracy: 0.9848 - val_loss: 0.0464\n",
            "Epoch 4/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 54ms/step - accuracy: 0.9906 - loss: 0.0286 - val_accuracy: 0.9840 - val_loss: 0.0483\n",
            "Epoch 5/5\n",
            "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 52ms/step - accuracy: 0.9932 - loss: 0.0222 - val_accuracy: 0.9895 - val_loss: 0.0305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modèle entraîné et sauvegardé avec succès !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "# Charger le modèle entraîné\n",
        "model = tf.keras.models.load_model('mnist_cnn_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDgUhgNE-ii1",
        "outputId": "1eb5a936-099e-455d-c466-215fde965423"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_data):\n",
        "    \"\"\"Prétraite l'image dessinée pour la prédiction\"\"\"\n",
        "    if image_data is None:\n",
        "        return None\n",
        "\n",
        "    # Gérer le cas où image_data est un dictionnaire (format Sketchpad)\n",
        "    if isinstance(image_data, dict):\n",
        "        if 'image' in image_data:\n",
        "            image = image_data['image']\n",
        "        elif 'composite' in image_data:\n",
        "            image = image_data['composite']\n",
        "        else:\n",
        "            # Prendre la première clé disponible qui contient une image\n",
        "            for key, value in image_data.items():\n",
        "                if isinstance(value, (Image.Image, np.ndarray)):\n",
        "                    image = value\n",
        "                    break\n",
        "            else:\n",
        "                return None\n",
        "    else:\n",
        "        image = image_data\n",
        "\n",
        "    # Convertir PIL Image en array numpy si nécessaire\n",
        "    if isinstance(image, Image.Image):\n",
        "        image = np.array(image)\n",
        "\n",
        "    # Vérifier que nous avons bien un array numpy\n",
        "    if not isinstance(image, np.ndarray):\n",
        "        return None\n",
        "\n",
        "    # Convertir en niveaux de gris si nécessaire\n",
        "    if len(image.shape) == 3:\n",
        "        if image.shape[2] == 4:  # RGBA\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2GRAY)\n",
        "        elif image.shape[2] == 3:  # RGB\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Redimensionner à 28x28 pixels\n",
        "    image = cv2.resize(image, (28, 28))\n",
        "\n",
        "    # Inverser les couleurs (MNIST a des chiffres blancs sur fond noir)\n",
        "    image = 255 - image\n",
        "\n",
        "    # Normaliser les valeurs entre 0 et 1\n",
        "    image = image.astype('float32') / 255.0\n",
        "\n",
        "    # Ajouter les dimensions pour le batch et le canal\n",
        "    image = image.reshape(1, 28, 28, 1)\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "uxcUE3em_csF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_digit(image_data):\n",
        "    \"\"\"Prédit le chiffre dessiné\"\"\"\n",
        "    try:\n",
        "        if image_data is None:\n",
        "            return \"Veuillez dessiner un chiffre\"\n",
        "\n",
        "        # Debug: afficher le type de données reçues\n",
        "        print(f\"Type de données reçues: {type(image_data)}\")\n",
        "        if isinstance(image_data, dict):\n",
        "            print(f\"Clés du dictionnaire: {list(image_data.keys())}\")\n",
        "\n",
        "        # Prétraiter l'image\n",
        "        processed_image = preprocess_image(image_data)\n",
        "\n",
        "        if processed_image is None:\n",
        "            return \"Erreur lors du prétraitement de l'image\"\n",
        "\n",
        "        # Faire la prédiction\n",
        "        prediction = model.predict(processed_image, verbose=0)\n",
        "        predicted_digit = np.argmax(prediction)\n",
        "        confidence = np.max(prediction) * 100\n",
        "\n",
        "        # Créer le résultat détaillé\n",
        "        result = f\"Chiffre prédit: {predicted_digit}\\nConfiance: {confidence:.2f}%\\n\\n\"\n",
        "        result += \"Probabilités pour chaque chiffre:\\n\"\n",
        "        for i in range(10):\n",
        "            prob = prediction[0][i] * 100\n",
        "            result += f\"{i}: {prob:.2f}%\\n\"\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "      return f\"Erreur: {str(e)}\\nType de données: {type(image_data)}\""
      ],
      "metadata": {
        "id": "6ga4aooV__Ca"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer l'interface Gradio\n",
        "with gr.Blocks(title=\"Reconnaissance de Chiffres MNIST\") as demo:\n",
        "    gr.Markdown(\"Reconnaissance de Chiffres MNIST\")\n",
        "    gr.Markdown(\"Dessinez un chiffre (0-9) dans la zone ci-dessous et cliquez sur 'Prédire' pour voir le résultat!\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            # Zone de dessin - utiliser type=\"numpy\" pour obtenir directement un array\n",
        "            sketchpad = gr.Sketchpad(\n",
        "                label=\"Dessinez un chiffre ici\",\n",
        "                type=\"numpy\",\n",
        "                canvas_size=(280, 280)\n",
        "            )\n",
        "\n",
        "            # Boutons\n",
        "            with gr.Row():\n",
        "                predict_btn = gr.Button(\"Prédire\", variant=\"primary\")\n",
        "                clear_btn = gr.Button(\"Effacer\")\n",
        "\n",
        "        with gr.Column():\n",
        "            # Résultats\n",
        "            result_text = gr.Textbox(\n",
        "                label=\"Résultat de la prédiction\",\n",
        "                lines=15,\n",
        "                interactive=False\n",
        "            )\n",
        "\n",
        "    # Fonction pour effacer le canvas\n",
        "    def clear_canvas():\n",
        "        return None, \"\"\n",
        "\n",
        "    # Événements\n",
        "    predict_btn.click(\n",
        "        fn=predict_digit,\n",
        "        inputs=[sketchpad],\n",
        "        outputs=[result_text]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=clear_canvas,\n",
        "        outputs=[sketchpad, result_text]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "YTGPa_JsAand",
        "outputId": "54fefa72-39f8-4c17-c279-0f29569bdc55"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f01a8172a6976aacea.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f01a8172a6976aacea.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GIcOts0ID57",
        "outputId": "03f4455c-573e-41cd-cea9-7e5ed8101053"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TAwFFLeHIE-y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}